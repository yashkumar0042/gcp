Here are detailed **hands-on labs** that align with the key considerations for planning a data warehouse. These labs provide practical experience with designing, implementing, and managing a data warehouse on platforms like Google Cloud Platform (GCP) or any other cloud/data warehouse tool.

---

### **Lab 1: Designing the Data Model**
**Objective**: Create a star schema for a business scenario using dimensional modeling techniques.

#### **Steps**:
1. **Setup**:
   - Use a relational database (e.g., BigQuery, PostgreSQL, or MySQL).
   - Create a database for your data warehouse project (e.g., `retail_dw`).

2. **Scenario**:
   - Business Process: Sales.
   - Dimensions: Products, Customers, Time.
   - Fact Table: Sales Transactions.

3. **Tasks**:
   - Design and implement the schema:
     ```sql
     CREATE TABLE Product_Dimension (
         ProductID INT PRIMARY KEY,
         ProductName VARCHAR(255),
         Category VARCHAR(255),
         Brand VARCHAR(255)
     );

     CREATE TABLE Customer_Dimension (
         CustomerID INT PRIMARY KEY,
         CustomerName VARCHAR(255),
         Gender VARCHAR(10),
         Region VARCHAR(255)
     );

     CREATE TABLE Time_Dimension (
         TimeID INT PRIMARY KEY,
         Date DATE,
         DayOfWeek VARCHAR(20),
         Month VARCHAR(20),
         Year INT
     );

     CREATE TABLE Sales_Fact (
         SaleID INT PRIMARY KEY,
         ProductID INT,
         CustomerID INT,
         TimeID INT,
         QuantitySold INT,
         TotalRevenue FLOAT,
         FOREIGN KEY (ProductID) REFERENCES Product_Dimension(ProductID),
         FOREIGN KEY (CustomerID) REFERENCES Customer_Dimension(CustomerID),
         FOREIGN KEY (TimeID) REFERENCES Time_Dimension(TimeID)
     );
     ```
   - Populate tables with sample data using `INSERT` statements.

4. **Verification**:
   - Query the database to ensure data joins work correctly:
     ```sql
     SELECT 
         p.ProductName, 
         c.CustomerName, 
         t.Date, 
         s.QuantitySold, 
         s.TotalRevenue
     FROM Sales_Fact s
     JOIN Product_Dimension p ON s.ProductID = p.ProductID
     JOIN Customer_Dimension c ON s.CustomerID = c.CustomerID
     JOIN Time_Dimension t ON s.TimeID = t.TimeID;
     ```

---

### **Lab 2: Deciding the Degree of Data Normalization**
**Objective**: Explore normalized and denormalized schemas and analyze query performance.

#### **Steps**:
1. **Setup**:
   - Extend the schema created in **Lab 1**.

2. **Tasks**:
   - Create a normalized schema for the `Customer_Dimension`:
     ```sql
     CREATE TABLE Region (
         RegionID INT PRIMARY KEY,
         RegionName VARCHAR(255)
     );

     CREATE TABLE Customer (
         CustomerID INT PRIMARY KEY,
         CustomerName VARCHAR(255),
         Gender VARCHAR(10),
         RegionID INT,
         FOREIGN KEY (RegionID) REFERENCES Region(RegionID)
     );
     ```

   - Populate the normalized schema with sample data.

3. **Comparison**:
   - Query the normalized schema:
     ```sql
     SELECT c.CustomerName, r.RegionName 
     FROM Customer c
     JOIN Region r ON c.RegionID = r.RegionID;
     ```
   - Query the original denormalized schema.

4. **Analyze**:
   - Compare query execution time for normalized vs. denormalized queries.
   - Discuss trade-offs in query complexity, performance, and storage.

---

### **Lab 3: Mapping Business Requirements**
**Objective**: Translate business requirements into a logical data model and create a priority-based implementation plan.

#### **Steps**:
1. **Scenario**:
   - The company needs a dashboard to analyze monthly sales trends and top-performing products.

2. **Tasks**:
   - Define metrics and KPIs:
     - Monthly sales revenue.
     - Top 5 products by revenue.

   - Create a logical data model:
     - Fact Table: `Sales_Fact`.
     - Dimensions: `Product_Dimension`, `Time_Dimension`.

   - Write SQL queries to satisfy the requirements:
     ```sql
     -- Monthly Sales Revenue
     SELECT 
         t.Year, 
         t.Month, 
         SUM(s.TotalRevenue) AS MonthlyRevenue
     FROM Sales_Fact s
     JOIN Time_Dimension t ON s.TimeID = t.TimeID
     GROUP BY t.Year, t.Month;

     -- Top 5 Products by Revenue
     SELECT 
         p.ProductName, 
         SUM(s.TotalRevenue) AS TotalRevenue
     FROM Sales_Fact s
     JOIN Product_Dimension p ON s.ProductID = p.ProductID
     GROUP BY p.ProductName
     ORDER BY TotalRevenue DESC
     LIMIT 5;
     ```

3. **Deliverable**:
   - Validate that the queries meet business requirements.

---

### **Lab 4: Defining Architecture to Support Data Access Patterns**
**Objective**: Design a data warehouse architecture that supports performance and scalability.

#### **Steps**:
1. **Setup**:
   - Use GCP BigQuery (or equivalent cloud data warehouse).

2. **Tasks**:
   - **Data Ingestion**:
     - Simulate batch ingestion:
       - Upload CSV files to Cloud Storage.
       - Use BigQuery to load the data:
         ```bash
         bq load --source_format=CSV my_dataset.Sales_Fact gs://bucket_name/sales_data.csv
         ```

   - **Partitioning and Clustering**:
     - Partition `Sales_Fact` by `TimeID`:
       ```sql
       CREATE TABLE Sales_Fact_Partitioned
       PARTITION BY Date(Date)
       AS SELECT * FROM Sales_Fact;
       ```
     - Cluster by `ProductID` and `CustomerID` for better performance:
       ```sql
       CREATE TABLE Sales_Fact_Clustered
       PARTITION BY Date(Date)
       CLUSTER BY ProductID, CustomerID
       AS SELECT * FROM Sales_Fact;
       ```

   - **Query Optimization**:
     - Create materialized views for frequently queried data:
       ```sql
       CREATE MATERIALIZED VIEW Top_Products AS
       SELECT 
           ProductID, 
           SUM(TotalRevenue) AS Revenue
       FROM Sales_Fact
       GROUP BY ProductID;
       ```

3. **Verification**:
   - Query partitioned and clustered tables to analyze performance improvements.

---

### **Tools and Technologies for Labs**
- **Cloud Platform**: GCP BigQuery, AWS Redshift, or Azure Synapse Analytics.
- **Database**: PostgreSQL/MySQL (for on-premise practice).
- **ETL Tools**: Apache Airflow, Dataflow, or cloud-native ETL solutions.
- **BI Tools**: Tableau, Power BI, or Looker for data visualization.

---

### **Lab Deliverables**
1. SQL scripts for creating and populating tables.
2. Query performance analysis reports.
3. Documentation of architecture design decisions and how they align with business requirements. 

These labs provide a practical approach to understanding and implementing data warehouse design principles while ensuring alignment with real-world business scenarios.
