Hereâ€™s an extended and detailed theoretical explanation along with step-by-step guidance for performing labs using **CLI** and **GUI** for the topic of **Sharing Data** in GCP Data Engineering.

---

## **1. Defining Rules to Share Data**

### **Theory:**
Sharing data in GCP requires strict adherence to security, governance, and compliance. It involves defining policies for access, monitoring usage, and ensuring compliance with regulations like GDPR and HIPAA.

#### **Key Concepts:**
- **IAM Roles and Permissions:**
  - Predefined roles like `roles/bigquery.dataViewer` or custom roles for tailored access.
- **Principle of Least Privilege:**
  - Grant only the necessary permissions to minimize exposure.
- **Encryption:**
  - All data is encrypted by default (at rest and in transit). Use CMEK (Customer-Managed Encryption Keys) for higher security.
- **Service Accounts:**
  - Use service accounts for programmatic access.
- **Data Masking:**
  - Use Cloud DLP to tokenize or redact sensitive data before sharing.

---

### **Lab 1: Controlling Access to BigQuery Datasets**

#### **Objective:**
Manage access to a BigQuery dataset using IAM.

#### **Steps Using GUI:**
1. **Create a Dataset:**
   - Navigate to the **BigQuery** console.
   - Select a project and click **+ CREATE DATASET**.
   - Provide a **Dataset ID** and adjust settings as required.

2. **Set Permissions:**
   - In the BigQuery console, locate your dataset.
   - Click **SHARING** or **Permissions**.
   - Add specific users/groups or service accounts, and assign roles (e.g., `BigQuery Data Viewer`).

3. **Test Access:**
   - Log in with the added user and attempt to query the dataset.

#### **Steps Using CLI:**
1. **Create a Dataset:**
   ```bash
   bq --location=US mk -d my_project_id:my_dataset_id
   ```

2. **Set IAM Policies:**
   ```bash
   gcloud projects add-iam-policy-binding my_project_id \
       --member="user:example_user@gmail.com" \
       --role="roles/bigquery.dataViewer"
   ```

3. **Test Access:**
   - Use `bq` commands or Google Cloud Console to verify access.

---

### **Lab 2: Data Masking with Cloud DLP**

#### **Objective:**
Mask sensitive data using Cloud DLP and share the masked dataset.

#### **Steps Using GUI:**
1. **Enable Cloud DLP:**
   - Go to **APIs & Services** and enable **Cloud Data Loss Prevention API**.

2. **Create a DLP Job:**
   - Navigate to **Data Loss Prevention** in the console.
   - Create a new job, selecting BigQuery as the source.
   - Define an infoType detector (e.g., "CREDIT_CARD_NUMBER").
   - Apply masking or tokenization.

3. **Share Masked Dataset:**
   - Publish the masked dataset and set appropriate permissions.

#### **Steps Using CLI:**
1. **Create a DLP Job:**
   ```bash
   gcloud dlp jobs create --project=my_project_id \
       --inspect-config='{"infoTypes":[{"name":"CREDIT_CARD_NUMBER"}]}' \
       --storage-config='{"bigQueryOptions":{"tableReference":{"projectId":"my_project_id","datasetId":"my_dataset","tableId":"my_table"}}}' \
       --actions='{"saveFindings":{"outputConfig":{"table":{"projectId":"my_project_id","datasetId":"masked_dataset","tableId":"masked_table"}}}}'
   ```

2. **Share Masked Dataset:**
   - Use BigQuery commands to manage permissions for the masked dataset.

---

## **2. Publishing Datasets**

### **Theory:**
Publishing datasets involves making data available to a wider audience or specific users while ensuring proper controls are in place.

#### **Key Concepts:**
- **Internal vs. External Sharing:**
  - Internal: Share within an organization.
  - External: Share datasets with partners or public.
- **Billing Implications:**
  - Ensure that the appropriate billing project is set up for external queries.
- **Documentation:**
  - Include metadata and schema descriptions for ease of use.

---

### **Lab 3: Publishing a Public Dataset on BigQuery**

#### **Objective:**
Publish a BigQuery dataset and make it publicly accessible.

#### **Steps Using GUI:**
1. **Create a Dataset:**
   - Navigate to the **BigQuery Console** and create a dataset.

2. **Set Public Access:**
   - Click **SHARING** and add `allUsers` with the `BigQuery Data Viewer` role.

3. **Verify Access:**
   - Log out and attempt to access the dataset via the public URL.

#### **Steps Using CLI:**
1. **Create a Dataset:**
   ```bash
   bq --location=US mk -d my_project_id:public_dataset
   ```

2. **Make Dataset Public:**
   ```bash
   gcloud projects add-iam-policy-binding my_project_id \
       --member="allUsers" \
       --role="roles/bigquery.dataViewer"
   ```

3. **Verify Access:**
   - Query the dataset from another GCP project or using `bq`.

---

## **3. Publishing Reports and Visualizations**

### **Theory:**
Reports and visualizations provide an accessible way to interpret and share data. Tools like **Looker Studio** enable creating interactive dashboards.

#### **Key Concepts:**
- **Data Sources:**
  - Use BigQuery datasets or Cloud Storage files.
- **Collaboration:**
  - Share reports with users via email or public links.
- **Embedding:**
  - Embed dashboards into web applications using iFrames or APIs.

---

### **Lab 5: Creating and Sharing Looker Studio Dashboards**

#### **Objective:**
Create and share a Looker Studio dashboard using a BigQuery dataset.

#### **Steps Using GUI:**
1. **Connect to BigQuery:**
   - Open **Looker Studio** and create a new report.
   - Select BigQuery as the data source and connect your dataset.

2. **Design the Dashboard:**
   - Add charts, filters, and visuals.
   - Optimize query execution using pre-aggregated data.

3. **Share the Dashboard:**
   - Click **SHARE** and set access levels.

#### **Steps Using CLI:**
While Looker Studio does not have CLI support, use GCP CLI for data preparation:
1. **Export Pre-Aggregated Data:**
   ```bash
   bq query --nouse_legacy_sql \
       'SELECT category, COUNT(*) as total FROM my_dataset.my_table GROUP BY category'
   ```

2. **Upload Results to Looker Studio:**
   - Export the query result to Google Sheets or CSV for connection.

---

## **4. Analytics Hub**

### **Theory:**
Analytics Hub simplifies data sharing across organizations by enabling data exchanges, subscriptions, and monitoring.

#### **Key Concepts:**
- **Data Exchanges:**
  - Centralized repositories for shared datasets.
- **Subscription Model:**
  - Subscribe to datasets without duplicating data.

---

### **Lab 7: Creating a Data Exchange**

#### **Objective:**
Set up a data exchange using Analytics Hub.

#### **Steps Using GUI:**
1. **Enable Analytics Hub:**
   - Go to **APIs & Services** and enable **Analytics Hub API**.

2. **Create a Data Exchange:**
   - Navigate to Analytics Hub in the console.
   - Click **CREATE DATA EXCHANGE** and add datasets.

3. **Publish Data:**
   - Add datasets to the exchange and configure access.

#### **Steps Using CLI:**
1. **Create a Data Exchange:**
   ```bash
   gcloud analytics-hub exchanges create my_exchange --project=my_project_id
   ```

2. **Publish a Dataset:**
   ```bash
   gcloud analytics-hub exchanges add-dataset my_exchange \
       --dataset=my_project_id.my_dataset_id
   ```

---

This structured approach ensures you gain both theoretical knowledge and practical experience using CLI and GUI. Let me know if you'd like code snippets or additional guidance!
